#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çˆ¬è™«ä¿®å¤è„šæœ¬
ä¿®å¤å„ä¸ªçˆ¬è™«ä¸­çš„å·²çŸ¥é—®é¢˜
"""

import os
import sys
import re

def fix_wallstreet_crawler():
    """ä¿®å¤åå°”è¡—è§é—»çˆ¬è™«çš„NoneTypeè¿­ä»£é—®é¢˜"""
    file_path = "newsnow-python/crawlers/wallstreet.py"
    
    if not os.path.exists(file_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return False
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ä¿®å¤asset_tagsçš„NoneTypeé—®é¢˜
        old_pattern = r'for tag in article_data\.get\("asset_tags", \[\]\):'
        new_pattern = 'for tag in (article_data.get("asset_tags") or []):'
        
        if re.search(old_pattern, content):
            content = re.sub(old_pattern, new_pattern, content)
            print("âœ… ä¿®å¤äº†asset_tagsçš„NoneTypeè¿­ä»£é—®é¢˜")
        
        # æ·»åŠ æ›´å¤šçš„ç©ºå€¼æ£€æŸ¥
        fixes = [
            # ä¿®å¤author_infoçš„NoneTypeé—®é¢˜
            (
                r'author_info = article_data\.get\("author", \{\}\)',
                'author_info = article_data.get("author") or {}'
            ),
            # ä¿®å¤source_infoçš„NoneTypeé—®é¢˜
            (
                r'source_info = article_data\.get\("source_info", \{\}\)',
                'source_info = article_data.get("source_info") or {}'
            ),
            # ä¿®å¤resourceçš„NoneTypeé—®é¢˜
            (
                r'resource = article_data\.get\("resource", \{\}\)',
                'resource = article_data.get("resource") or {}'
            )
        ]
        
        for old, new in fixes:
            if re.search(old, content):
                content = re.sub(old, new, content)
                print(f"âœ… åº”ç”¨ä¿®å¤: {old[:30]}...")
        
        # æ·»åŠ æ›´å®‰å…¨çš„æ—¶é—´æˆ³å¤„ç†
        timestamp_fix = '''
            # æå–å‘å¸ƒæ—¶é—´ - å®‰å…¨å¤„ç†
            pub_timestamp = article_data.get("display_time") or article_data.get("published_at") or 0
            try:
                if pub_timestamp and pub_timestamp > 0:
                    # å¤„ç†æ¯«ç§’æ—¶é—´æˆ³
                    if pub_timestamp > 1000000000000:  # æ¯«ç§’æ—¶é—´æˆ³
                        pub_date = datetime.fromtimestamp(pub_timestamp/1000).isoformat()
                    else:  # ç§’æ—¶é—´æˆ³
                        pub_date = datetime.fromtimestamp(pub_timestamp).isoformat()
                else:
                    pub_date = datetime.now().isoformat()
            except (ValueError, OSError) as e:
                print(f"[Wallstreet Warn] Article ID: {article_id} - æ—¶é—´æˆ³è§£æå¤±è´¥: {e}")
                pub_date = datetime.now().isoformat()
'''
        
        # æ›¿æ¢åŸæœ‰çš„æ—¶é—´æˆ³å¤„ç†é€»è¾‘
        old_timestamp_pattern = r'# æå–å‘å¸ƒæ—¶é—´\s+pub_timestamp = article_data\.get\("display_time", 0\).*?pub_date = datetime\.fromtimestamp\(pub_timestamp/1000\)\.isoformat\(\) if pub_timestamp else ""'
        
        if re.search(old_timestamp_pattern, content, re.DOTALL):
            content = re.sub(old_timestamp_pattern, timestamp_fix.strip(), content, flags=re.DOTALL)
            print("âœ… ä¿®å¤äº†æ—¶é—´æˆ³å¤„ç†é€»è¾‘")
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"âœ… åå°”è¡—è§é—»çˆ¬è™«ä¿®å¤å®Œæˆ: {file_path}")
        return True
        
    except Exception as e:
        print(f"âŒ ä¿®å¤åå°”è¡—è§é—»çˆ¬è™«å¤±è´¥: {e}")
        return False

def fix_gelonghui_crawler():
    """ä¿®å¤æ ¼éš†æ±‡çˆ¬è™«çš„URLé—®é¢˜"""
    file_path = "newsnow-python/crawlers/gelonghui.py"
    
    if not os.path.exists(file_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return False
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æ·»åŠ URLéªŒè¯å’Œé‡è¯•é€»è¾‘
        url_fix = '''
        # æ„å»ºæ–‡ç« è¯¦æƒ…URL - æ”¯æŒå¤šç§æ ¼å¼
        possible_urls = [
            f"{self.base_url}/p/{article_id}",
            f"{self.base_url}/live/{article_id}",
            f"{self.base_url}/news/{article_id}",
            f"{self.base_url}/article/{article_id}"
        ]
        
        response = None
        url = None
        
        # å°è¯•ä¸åŒçš„URLæ ¼å¼
        for test_url in possible_urls:
            try:
                print(f"[Gelonghui Debug] Article ID: {article_id} - Trying URL: {test_url}")
                test_response = requests.get(
                    test_url,
                    headers=self.headers,
                    timeout=REQUEST_TIMEOUT,
                    allow_redirects=True
                )
                
                if test_response.status_code == 200:
                    response = test_response
                    url = test_url
                    print(f"[Gelonghui Debug] Article ID: {article_id} - Found working URL: {url}")
                    break
                else:
                    print(f"[Gelonghui Debug] Article ID: {article_id} - URL failed with status {test_response.status_code}: {test_url}")
                    
            except Exception as e:
                print(f"[Gelonghui Debug] Article ID: {article_id} - URL request failed: {test_url} - {e}")
                continue
        
        if not response or response.status_code != 200:
            print(f"[Gelonghui Error] Article ID: {article_id} - All URL attempts failed")
            return None
'''
        
        # æŸ¥æ‰¾å¹¶æ›¿æ¢åŸæœ‰çš„URLæ„å»ºé€»è¾‘
        old_url_pattern = r'url = f"\{self\.base_url\}/p/\{article_id\}".*?if response\.status_code != 200:'
        
        if re.search(old_url_pattern, content, re.DOTALL):
            # æ‰¾åˆ°åŒ¹é…çš„éƒ¨åˆ†å¹¶æ›¿æ¢
            content = re.sub(
                r'url = f"\{self\.base_url\}/p/\{article_id\}".*?response = requests\.get\(.*?\).*?print\(f"\[Gelonghui Debug\] Article ID: \{article_id\} - HTTP Status: \{response\.status_code\}"\).*?if response\.status_code != 200:',
                url_fix.strip() + '\n\n        print(f"[Gelonghui Debug] Article ID: {article_id} - HTTP Status: {response.status_code}")\n        if response.status_code != 200:',
                content,
                flags=re.DOTALL
            )
            print("âœ… ä¿®å¤äº†æ ¼éš†æ±‡URLå¤„ç†é€»è¾‘")
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"âœ… æ ¼éš†æ±‡çˆ¬è™«ä¿®å¤å®Œæˆ: {file_path}")
        return True
        
    except Exception as e:
        print(f"âŒ ä¿®å¤æ ¼éš†æ±‡çˆ¬è™«å¤±è´¥: {e}")
        return False

def fix_fastbull_crawler():
    """ä¿®å¤FastBullçˆ¬è™«çš„å†…å®¹è§£æé—®é¢˜"""
    file_path = "newsnow-python/crawlers/fastbull.py"
    
    if not os.path.exists(file_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return False
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æ”¹è¿›å†…å®¹é€‰æ‹©å™¨
        selector_fix = '''
            # å°è¯•å¤šç§å†…å®¹é€‰æ‹©å™¨
            content_selectors = [
                '.news-detail-content',
                '.article-content', 
                '.content',
                '.news-content',
                '.detail-content',
                'article',
                '.main-content'
            ]
            
            content_elem = None
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem and content_elem.get_text(strip=True):
                    print(f"[FastBull Debug] Article ID: {article_id} - Found content with selector: {selector}")
                    break
            
            if content_elem:
                # æ¸…ç†å†…å®¹
                for script in content_elem(["script", "style", "nav", "footer", "header"]):
                    script.decompose()
                content = content_elem.get_text(strip=True)
                html_content = str(content_elem)
            else:
                print(f"[FastBull Warn] Article ID: {article_id} - No content found with any selector")
                content = ""
                html_content = ""
'''
        
        # æŸ¥æ‰¾å¹¶æ›¿æ¢å†…å®¹è§£æé€»è¾‘
        old_content_pattern = r'# å°è¯•è·å–æ–°é—»è¯¦æƒ…å†…å®¹.*?html_content = ""'
        
        if re.search(old_content_pattern, content, re.DOTALL):
            content = re.sub(old_content_pattern, selector_fix.strip(), content, flags=re.DOTALL)
            print("âœ… ä¿®å¤äº†FastBullå†…å®¹è§£æé€»è¾‘")
        
        # æ”¹è¿›æ ‡é¢˜æå–
        title_fix = '''
            # å°è¯•å¤šç§æ ‡é¢˜é€‰æ‹©å™¨
            title_selectors = [
                'h1.news-title',
                'h1.article-title', 
                '.title',
                'h1',
                '.news-detail-title',
                '.detail-title'
            ]
            
            title = ""
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem and title_elem.get_text(strip=True):
                    title = title_elem.get_text(strip=True)
                    print(f"[FastBull Debug] Article ID: {article_id} - Found title with selector: {selector}")
                    break
            
            if not title:
                # ä»URLæˆ–å…¶ä»–åœ°æ–¹å°è¯•è·å–æ ‡é¢˜
                title_elem = soup.select_one('title')
                if title_elem:
                    title = title_elem.get_text(strip=True)
                    # æ¸…ç†æ ‡é¢˜ä¸­çš„ç½‘ç«™åç§°
                    title = re.sub(r'[-_|].*?(FastBull|å¿«ç‰›).*$', '', title).strip()
'''
        
        # æŸ¥æ‰¾å¹¶æ›¿æ¢æ ‡é¢˜æå–é€»è¾‘
        old_title_pattern = r'# æå–æ ‡é¢˜.*?title = title_elem\.get_text\(strip=True\) if title_elem else ""'
        
        if re.search(old_title_pattern, content, re.DOTALL):
            content = re.sub(old_title_pattern, title_fix.strip(), content, flags=re.DOTALL)
            print("âœ… ä¿®å¤äº†FastBullæ ‡é¢˜æå–é€»è¾‘")
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"âœ… FastBullçˆ¬è™«ä¿®å¤å®Œæˆ: {file_path}")
        return True
        
    except Exception as e:
        print(f"âŒ ä¿®å¤FastBullçˆ¬è™«å¤±è´¥: {e}")
        return False

def fix_api_credentials():
    """ä¿®å¤APIå‡­è¯é—®é¢˜"""
    env_file = "newsnow-python/.env"
    env_example = "newsnow-python/env.example"
    
    print("\nğŸ”‘ APIå‡­è¯é…ç½®æ£€æŸ¥:")
    
    if not os.path.exists(env_file):
        if os.path.exists(env_example):
            print(f"âš ï¸  .envæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å¤åˆ¶ {env_example} åˆ° {env_file} å¹¶é…ç½®APIå¯†é’¥")
        else:
            print("âš ï¸  .envæ–‡ä»¶å’Œenv.exampleéƒ½ä¸å­˜åœ¨ï¼Œè¯·æ‰‹åŠ¨åˆ›å»º.envæ–‡ä»¶")
        return False
    
    try:
        with open(env_file, 'r', encoding='utf-8') as f:
            env_content = f.read()
        
        # æ£€æŸ¥å¿…è¦çš„APIå¯†é’¥
        required_keys = [
            'DEEPSEEK_API_KEY',
            'OPENAI_API_KEY',
            'SEARXNG_URL'
        ]
        
        missing_keys = []
        for key in required_keys:
            if f"{key}=" not in env_content or f"{key}=" in env_content and not re.search(f"{key}=.+", env_content):
                missing_keys.append(key)
        
        if missing_keys:
            print(f"âš ï¸  ç¼ºå°‘æˆ–æœªé…ç½®çš„APIå¯†é’¥: {', '.join(missing_keys)}")
            print("è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®è¿™äº›å¯†é’¥ä»¥å¯ç”¨AIåˆ†æåŠŸèƒ½")
            return False
        else:
            print("âœ… APIå¯†é’¥é…ç½®æ£€æŸ¥é€šè¿‡")
            return True
            
    except Exception as e:
        print(f"âŒ æ£€æŸ¥APIå‡­è¯å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ å¼€å§‹ä¿®å¤çˆ¬è™«é—®é¢˜...")
    print("=" * 60)
    
    # ä¿®å¤å„ä¸ªçˆ¬è™«
    fixes = [
        ("åå°”è¡—è§é—»çˆ¬è™«", fix_wallstreet_crawler),
        ("æ ¼éš†æ±‡çˆ¬è™«", fix_gelonghui_crawler), 
        ("FastBullçˆ¬è™«", fix_fastbull_crawler),
    ]
    
    success_count = 0
    for name, fix_func in fixes:
        print(f"\nğŸ”§ ä¿®å¤ {name}...")
        if fix_func():
            success_count += 1
        else:
            print(f"âŒ {name} ä¿®å¤å¤±è´¥")
    
    print(f"\nğŸ“Š ä¿®å¤ç»“æœ: {success_count}/{len(fixes)} ä¸ªçˆ¬è™«ä¿®å¤æˆåŠŸ")
    
    # æ£€æŸ¥APIå‡­è¯
    fix_api_credentials()
    
    print("\n" + "=" * 60)
    print("ğŸ‰ çˆ¬è™«ä¿®å¤å®Œæˆï¼")
    print("\nğŸ’¡ å»ºè®®:")
    print("1. é…ç½®.envæ–‡ä»¶ä¸­çš„APIå¯†é’¥ä»¥å¯ç”¨AIåˆ†æ")
    print("2. é‡æ–°è¿è¡Œæµ‹è¯•: python3 test_all_crawlers.py")
    print("3. å¦‚æœä»æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œç›®æ ‡ç½‘ç«™çŠ¶æ€")

if __name__ == "__main__":
    main() 