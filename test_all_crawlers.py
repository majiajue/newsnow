#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•æ‰€æœ‰çˆ¬è™«çš„å®Œæ•´æµç¨‹
åŒ…æ‹¬æ–‡ç« è·å–ã€æœç´¢å¢å¼ºã€AIåˆ†æå’Œæ•°æ®åº“ä¿å­˜
"""

import sys
import os
import signal
import time
from contextlib import contextmanager

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_path = '/Users/majiajue/Desktop/newsnow/newsnow-python'
if project_path not in sys.path:
    sys.path.insert(0, project_path)

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['DEEPSEEK_API_KEY'] = 'sk-a4b8e8b6e8a04e5b8e8e8e8e8e8e8e8e'

@contextmanager
def timeout(duration):
    """è¶…æ—¶æ§åˆ¶ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    def timeout_handler(signum, frame):
        raise TimeoutError(f"æ“ä½œè¶…æ—¶ ({duration}ç§’)")
    
    signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(duration)
    try:
        yield
    finally:
        signal.alarm(0)

def test_crawler(crawler_class, crawler_name, source_name):
    """æµ‹è¯•å•ä¸ªçˆ¬è™«"""
    print(f"\n{'='*60}")
    print(f"æµ‹è¯• {crawler_name} ({source_name})")
    print(f"{'='*60}")
    
    try:
        # 1. åˆå§‹åŒ–çˆ¬è™«
        print("1. åˆå§‹åŒ–çˆ¬è™«...")
        with timeout(30):
            crawler = crawler_class()
        print(f"âœ… {crawler_name} åˆå§‹åŒ–æˆåŠŸ")
        
        # 2. è·å–æœ€æ–°æ–‡ç« åˆ—è¡¨
        print("2. è·å–æœ€æ–°æ–‡ç« åˆ—è¡¨...")
        with timeout(30):
            # æ ¹æ®çˆ¬è™«ç±»å‹é€‰æ‹©åˆé€‚çš„æ–¹æ³•
            if hasattr(crawler, 'get_latest_news'):
                articles = crawler.get_latest_news()
            elif hasattr(crawler, 'get_latest_articles'):
                articles = crawler.get_latest_articles()
            else:
                print(f"âš ï¸ {crawler_name} æ²¡æœ‰è·å–æ–‡ç« åˆ—è¡¨çš„æ–¹æ³•")
                return False
        
        if not articles:
            print(f"âš ï¸ {crawler_name} æœªè·å–åˆ°æ–‡ç« ")
            return False
            
        print(f"âœ… è·å–åˆ° {len(articles)} æ¡æ–‡ç« ")
        
        # æ˜¾ç¤ºç¬¬ä¸€ç¯‡æ–‡ç« ä¿¡æ¯
        first_article = articles[0]
        if isinstance(first_article, dict):
            article_id = first_article.get('id', 'Unknown')
            title = first_article.get('title', 'No title')[:50] + '...' if len(first_article.get('title', '')) > 50 else first_article.get('title', 'No title')
        else:
            article_id = str(first_article)
            title = "Unknown"
        
        print(f"æµ‹è¯•æ–‡ç« ID: {article_id}")
        print(f"æ–‡ç« æ ‡é¢˜: {title}")
        
        # 3. æµ‹è¯•æ–‡ç« è¯¦æƒ…è·å–
        print("3. æµ‹è¯•æ–‡ç« è¯¦æƒ…è·å–...")
        with timeout(60):
            if hasattr(crawler, 'get_article_detail'):
                result = crawler.get_article_detail(article_id)
                if result:
                    print(f"âœ… {crawler_name} æ–‡ç« è¯¦æƒ…è·å–æˆåŠŸ")
                    if isinstance(result, dict):
                        print(f"   æ ‡é¢˜: {result.get('title', 'No title')[:50]}...")
                        print(f"   æ˜¯å¦åŒ…å«AIåˆ†æ: {bool(result.get('analysis'))}")
                        print(f"   æ˜¯å¦ç«‹å³å¤„ç†: {result.get('processed_immediately', False)}")
                    return True
                else:
                    print(f"âŒ {crawler_name} æ–‡ç« è¯¦æƒ…è·å–å¤±è´¥")
                    return False
            else:
                print(f"âš ï¸ {crawler_name} æ²¡æœ‰ get_article_detail æ–¹æ³•")
                return False
                
    except TimeoutError as e:
        print(f"âŒ {crawler_name} æµ‹è¯•è¶…æ—¶: {e}")
        return False
    except Exception as e:
        print(f"âŒ {crawler_name} æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=== æµ‹è¯•æ‰€æœ‰çˆ¬è™«çš„å®Œæ•´æµç¨‹ ===")
    
    # çˆ¬è™«é…ç½®
    crawlers_config = [
        {
            'module': 'crawlers.jin10',
            'class': 'Jin10Crawler',
            'name': 'Jin10çˆ¬è™«',
            'source': 'é‡‘åæ•°æ®'
        },
        {
            'module': 'crawlers.gelonghui',
            'class': 'GelonghuiCrawler',
            'name': 'Gelonghuiçˆ¬è™«',
            'source': 'æ ¼éš†æ±‡'
        },
        {
            'module': 'crawlers.fastbull',
            'class': 'FastbullCrawler',  # æ³¨æ„è¿™é‡Œæ˜¯å°å†™b
            'name': 'FastBullçˆ¬è™«',
            'source': 'FastBull'
        },
        {
            'module': 'crawlers.wallstreet',
            'class': 'WallstreetCrawler',
            'name': 'Wallstreetçˆ¬è™«',
            'source': 'åå°”è¡—è§é—»'
        }
    ]
    
    results = {}
    
    for config in crawlers_config:
        try:
            # åŠ¨æ€å¯¼å…¥çˆ¬è™«ç±»
            module = __import__(config['module'], fromlist=[config['class']])
            crawler_class = getattr(module, config['class'])
            
            # æµ‹è¯•çˆ¬è™«
            success = test_crawler(crawler_class, config['name'], config['source'])
            results[config['name']] = success
            
        except ImportError as e:
            print(f"âŒ æ— æ³•å¯¼å…¥ {config['name']}: {e}")
            results[config['name']] = False
        except Exception as e:
            print(f"âŒ {config['name']} æµ‹è¯•å¼‚å¸¸: {e}")
            results[config['name']] = False
    
    # æ˜¾ç¤ºæ€»ç»“
    print(f"\n{'='*60}")
    print("æµ‹è¯•ç»“æœæ€»ç»“")
    print(f"{'='*60}")
    
    success_count = 0
    for crawler_name, success in results.items():
        status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
        print(f"{crawler_name}: {status}")
        if success:
            success_count += 1
    
    print(f"\næ€»è®¡: {success_count}/{len(results)} ä¸ªçˆ¬è™«æµ‹è¯•æˆåŠŸ")
    
    if success_count == len(results):
        print("ğŸ‰ æ‰€æœ‰çˆ¬è™«éƒ½æ­£å¸¸å·¥ä½œï¼")
    else:
        print("âš ï¸ éƒ¨åˆ†çˆ¬è™«éœ€è¦ä¿®å¤")

if __name__ == "__main__":
    main()
