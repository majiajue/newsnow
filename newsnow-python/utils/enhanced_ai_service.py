#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆAIåˆ†ææœåŠ¡ - ä¸“ä¸ºAdSenseå®¡æŸ¥ä¼˜åŒ–
æä¾›é«˜è´¨é‡çš„åŸåˆ›è´¢ç»åˆ†æå†…å®¹
"""

import os
import json
import time
import requests
from datetime import datetime
import hashlib
import logging

logger = logging.getLogger(__name__)

class EnhancedFinanceAnalyzer:
    """å¢å¼ºç‰ˆè´¢ç»åˆ†æå™¨ - ä¸“ä¸ºå†…å®¹è´¨é‡ä¼˜åŒ–"""
    
    def __init__(self, api_key=None):
        self.api_key = api_key or os.environ.get("DEEPSEEK_API_KEY")
        self.api_url = "https://api.deepseek.com/v1/chat/completions"
        self.cache = {}
        self.cache_ttl = 1800  # 30åˆ†é’Ÿç¼“å­˜
        self.last_request_time = 0
        self.min_request_interval = 8  # æœ€å°è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
    
    def _wait_if_needed(self):
        """ç®€å•çš„è¯·æ±‚é¢‘ç‡æ§åˆ¶"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        if time_since_last < self.min_request_interval:
            wait_time = self.min_request_interval - time_since_last
            logger.info(f"[AI] ğŸ• ç­‰å¾… {wait_time:.1f} ç§’ä»¥é¿å…é¢‘ç‡é™åˆ¶...")
            time.sleep(wait_time)
        
        self.last_request_time = time.time()
    
    def _call_api_with_retry(self, prompt, system_prompt=None, max_retries=5):
        """å¸¦æ™ºèƒ½é‡è¯•æœºåˆ¶çš„APIè°ƒç”¨ - ä¼˜åŒ–é™æµå¤„ç†"""
        base_delay = 2  # åŸºç¡€å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        
        for attempt in range(max_retries):
            try:
                headers = {
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {self.api_key}",
                    "User-Agent": "NewsNow-AI-Analyzer/1.0"
                }
                
                messages = []
                if system_prompt:
                    messages.append({"role": "system", "content": system_prompt})
                messages.append({"role": "user", "content": prompt})
                
                payload = {
                    "model": "deepseek-chat",
                    "messages": messages,
                    "max_tokens": 1200,
                    "temperature": 0.7,
                    "top_p": 0.9
                }
                
                # åœ¨è¯·æ±‚å‰æ·»åŠ å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡è¿‡é«˜
                if attempt > 0:
                    delay = base_delay * (2 ** (attempt - 1))  # æŒ‡æ•°é€€é¿
                    logger.info(f"[AI] ç­‰å¾… {delay} ç§’åé‡è¯• (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(delay)
                
                # ä½¿ç”¨ç®€å•çš„è¯·æ±‚é™åˆ¶å™¨
                self._wait_if_needed()
                
                response = requests.post(
                    self.api_url, 
                    headers=headers, 
                    json=payload, 
                    timeout=60  # å¢åŠ è¶…æ—¶æ—¶é—´
                )
                
                if response.status_code == 200:
                    result = response.json()
                    content = result.get("choices", [{}])[0].get("message", {}).get("content", "")
                    logger.info(f"[AI] âœ… APIè°ƒç”¨æˆåŠŸ (å°è¯• {attempt + 1}/{max_retries})")
                    return {"success": True, "content": content}
                    
                elif response.status_code == 401:
                    error_msg = response.text
                    logger.error(f"[AI] âŒ APIè®¤è¯å¤±è´¥: {error_msg}")
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯å¤šæ¬¡401é”™è¯¯å¯¼è‡´çš„ä¸´æ—¶é”å®š
                    if "Multiple 401 errors detected" in error_msg:
                        if attempt < max_retries - 1:
                            wait_time = 70  # ç­‰å¾…70ç§’ï¼Œæ¯”è¦æ±‚çš„60ç§’å¤šä¸€ç‚¹
                            logger.warning(f"[AI] ğŸ• æ£€æµ‹åˆ°è®¤è¯é”å®šï¼Œç­‰å¾… {wait_time} ç§’...")
                            time.sleep(wait_time)
                            continue
                    
                    return {"success": False, "error": f"APIè®¤è¯å¤±è´¥: {error_msg}"}
                    
                elif response.status_code == 429:
                    error_msg = response.text
                    logger.warning(f"[AI] âš ï¸ APIè¯·æ±‚é¢‘ç‡è¿‡é«˜ (å°è¯• {attempt + 1}/{max_retries}): {error_msg}")
                    
                    if attempt < max_retries - 1:
                        # å¯¹äº429é”™è¯¯ï¼Œä½¿ç”¨æ›´é•¿çš„ç­‰å¾…æ—¶é—´
                        wait_time = 60 + (attempt * 30)  # 60, 90, 120, 150ç§’
                        logger.info(f"[AI] ğŸ• ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                        continue
                    else:
                        return {"success": False, "error": f"APIè¯·æ±‚é¢‘ç‡é™åˆ¶: {error_msg}"}
                        
                else:
                    logger.warning(f"[AI] âš ï¸ APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {response.status_code}")
                    if attempt == max_retries - 1:
                        return {"success": False, "error": f"APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}"}
                    
                    # å¯¹äºå…¶ä»–é”™è¯¯ï¼Œä½¿ç”¨è¾ƒçŸ­çš„ç­‰å¾…æ—¶é—´
                    time.sleep(base_delay * (attempt + 1))
                    
            except requests.exceptions.Timeout:
                logger.warning(f"[AI] â° APIè¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1}/{max_retries})")
                if attempt == max_retries - 1:
                    return {"success": False, "error": "APIè¯·æ±‚è¶…æ—¶"}
                time.sleep(base_delay * (attempt + 1))
                
            except Exception as e:
                logger.error(f"[AI] âŒ APIè°ƒç”¨å¼‚å¸¸ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt == max_retries - 1:
                    return {"success": False, "error": f"APIè°ƒç”¨å¼‚å¸¸: {str(e)}"}
                time.sleep(base_delay * (attempt + 1))
        
        return {"success": False, "error": "æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†"}
    
    def generate_comprehensive_analysis(self, title, content, search_results=None):
        """ç”Ÿæˆå…¨é¢çš„è´¢ç»åˆ†æ - AdSenseå‹å¥½"""
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = hashlib.md5(f"{title}_{content[:100]}".encode()).hexdigest()
        current_time = time.time()
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self.cache:
            cached_data, cached_time = self.cache[cache_key]
            if current_time - cached_time < self.cache_ttl:
                logger.info("[AI] ä½¿ç”¨ç¼“å­˜çš„åˆ†æç»“æœ")
                return cached_data
        
        system_prompt = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„è´¢ç»åˆ†æå¸ˆå’Œå†…å®¹åˆ›ä½œä¸“å®¶ï¼Œä¸“é—¨ä¸ºæ–°é—»ç½‘ç«™åˆ›ä½œé«˜è´¨é‡çš„åŸåˆ›åˆ†æå†…å®¹ã€‚

ä½ çš„ä»»åŠ¡æ˜¯åŸºäºæä¾›çš„æ–°é—»å†…å®¹ï¼Œåˆ›ä½œä¸€ç¯‡æ·±åº¦åˆ†ææ–‡ç« ï¼Œè¦æ±‚ï¼š

1. å†…å®¹åŸåˆ›æ€§ï¼šå®Œå…¨åŸåˆ›ï¼Œä¸å¾—æŠ„è¢­æˆ–ç®€å•æ”¹å†™
2. ä¸“ä¸šæ·±åº¦ï¼šæä¾›ä¸“ä¸šçš„è´¢ç»åˆ†æå’Œè§è§£
3. ç»“æ„æ¸…æ™°ï¼šåŒ…å«å¤šä¸ªåˆ†æç»´åº¦
4. ä»·å€¼å¯¼å‘ï¼šä¸ºè¯»è€…æä¾›å®ç”¨çš„æŠ•èµ„å‚è€ƒ
5. SEOå‹å¥½ï¼šåŒ…å«ç›¸å…³å…³é”®è¯å’Œæ ‡ç­¾

è¯·ç¡®ä¿å†…å®¹ç¬¦åˆä»¥ä¸‹æ ‡å‡†ï¼š
- å­—æ•°å……è¶³ï¼ˆ500-800å­—ï¼‰
- è§‚ç‚¹ç‹¬ç‰¹ä¸”æœ‰ä»·å€¼
- æ•°æ®æ”¯æ’‘çš„åˆ†æ
- é£é™©æç¤ºå’Œå…è´£å£°æ˜
- é€‚åˆæœç´¢å¼•æ“æ”¶å½•"""

        # æ„å»ºæœç´¢ç»“æœä¸Šä¸‹æ–‡
        search_context = ""
        if search_results and len(search_results) > 0:
            search_context = "\n\nç›¸å…³å¸‚åœºä¿¡æ¯ï¼š\n"
            for i, result in enumerate(search_results[:3], 1):
                search_context += f"{i}. {result.get('title', '')}: {result.get('content', '')[:100]}...\n"

        prompt = f"""è¯·åŸºäºä»¥ä¸‹æ–°é—»å†…å®¹åˆ›ä½œä¸€ç¯‡ä¸“ä¸šçš„è´¢ç»åˆ†ææ–‡ç« ï¼š

æ ‡é¢˜ï¼š{title}

æ–°é—»å†…å®¹ï¼š
{content}
{search_context}

è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š

```json
{{
  "analysis_title": "åˆ†ææ–‡ç« æ ‡é¢˜ï¼ˆä¸åŸæ ‡é¢˜ä¸åŒçš„åŸåˆ›æ ‡é¢˜ï¼‰",
  "executive_summary": "æ‰§è¡Œæ‘˜è¦ï¼ˆ100-150å­—ï¼‰",
  "market_analysis": {{
    "immediate_impact": "å³æ—¶å¸‚åœºå½±å“åˆ†æï¼ˆ150-200å­—ï¼‰",
    "long_term_implications": "é•¿æœŸå½±å“åˆ†æï¼ˆ150-200å­—ï¼‰",
    "affected_sectors": [
      {{
        "sector": "å—å½±å“è¡Œä¸š",
        "impact_level": "é«˜/ä¸­/ä½",
        "key_companies": ["å…¬å¸1", "å…¬å¸2"],
        "analysis": "å…·ä½“å½±å“åˆ†æ"
      }}
    ]
  }},
  "investment_perspective": {{
    "opportunities": "æŠ•èµ„æœºä¼šåˆ†æï¼ˆ100-150å­—ï¼‰",
    "risks": "é£é™©æç¤ºï¼ˆ100-150å­—ï¼‰",
    "strategy_suggestions": "ç­–ç•¥å»ºè®®ï¼ˆ100-150å­—ï¼‰"
  }},
  "technical_analysis": {{
    "key_indicators": "å…³é”®æŠ€æœ¯æŒ‡æ ‡åˆ†æ",
    "price_targets": "ä»·æ ¼ç›®æ ‡é¢„æµ‹",
    "support_resistance": "æ”¯æ’‘é˜»åŠ›ä½åˆ†æ"
  }},
  "conclusion": "ç»“è®ºå’Œå±•æœ›ï¼ˆ100-150å­—ï¼‰",
  "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2", "æ ‡ç­¾3", "æ ‡ç­¾4", "æ ‡ç­¾5"],
  "seo_keywords": ["å…³é”®è¯1", "å…³é”®è¯2", "å…³é”®è¯3"],
  "risk_disclaimer": "æŠ•èµ„é£é™©æç¤ºå’Œå…è´£å£°æ˜",
  "content_quality_score": 95,
  "originality_score": 98
}}
```

è¯·ç¡®ä¿è¿”å›çš„å†…å®¹å®Œå…¨åŸåˆ›ï¼Œå…·æœ‰ç‹¬ç‰¹çš„åˆ†æè§†è§’å’Œä»·å€¼ã€‚"""

        # è°ƒç”¨API
        result = self._call_api_with_retry(prompt, system_prompt)
        
        if not result["success"]:
            logger.error(f"[AI] åˆ†æå¤±è´¥: {result['error']}")
            return self._generate_fallback_analysis(title, content)
        
        # è§£æJSONå“åº”
        try:
            content_text = result["content"]
            logger.info(f"[AI] æ”¶åˆ°APIå“åº”ï¼Œé•¿åº¦: {len(content_text)} å­—ç¬¦")
            
            # å¤šç§æ–¹å¼æå–JSONéƒ¨åˆ†
            import re
            analysis_data = None
            
            # æ–¹å¼1: æ ‡å‡†çš„ ```json``` æ ¼å¼
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', content_text)
            if json_match:
                json_content = json_match.group(1).strip()
                logger.info("[AI] æ‰¾åˆ°æ ‡å‡†JSONæ ¼å¼")
                try:
                    analysis_data = json.loads(json_content)
                except json.JSONDecodeError as e:
                    logger.warning(f"[AI] æ ‡å‡†JSONè§£æå¤±è´¥: {e}")
            
            # æ–¹å¼2: å°è¯•æ‰¾åˆ°ä»»ä½• { } åŒ…å›´çš„JSON
            if not analysis_data:
                json_match = re.search(r'\{[\s\S]*\}', content_text)
                if json_match:
                    json_content = json_match.group(0).strip()
                    logger.info("[AI] æ‰¾åˆ°å¤§æ‹¬å·JSONæ ¼å¼")
                    try:
                        analysis_data = json.loads(json_content)
                    except json.JSONDecodeError as e:
                        logger.warning(f"[AI] å¤§æ‹¬å·JSONè§£æå¤±è´¥: {e}")
            
            # æ–¹å¼3: å°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”
            if not analysis_data:
                logger.info("[AI] å°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”")
                try:
                    analysis_data = json.loads(content_text.strip())
                except json.JSONDecodeError as e:
                    logger.warning(f"[AI] ç›´æ¥è§£æå¤±è´¥: {e}")
            
            if analysis_data:
                # æ·»åŠ å…ƒæ•°æ®
                analysis_data["generated_at"] = datetime.now().isoformat()
                analysis_data["ai_model"] = "deepseek-chat"
                analysis_data["analysis_version"] = "2.0"
                
                # ç¼“å­˜ç»“æœ
                self.cache[cache_key] = (analysis_data, current_time)
                
                logger.info("[AI] âœ… æˆåŠŸç”ŸæˆAIåˆ†æå†…å®¹")
                return analysis_data
            else:
                logger.warning("[AI] âš ï¸ æ‰€æœ‰JSONè§£ææ–¹å¼éƒ½å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
                # è®°å½•å‰200ä¸ªå­—ç¬¦ç”¨äºè°ƒè¯•
                logger.debug(f"[AI] å“åº”å†…å®¹å‰200å­—ç¬¦: {content_text[:200]}...")
                return self._generate_fallback_analysis(title, content)
                
        except Exception as e:
            logger.error(f"[AI] JSONè§£æå¼‚å¸¸: {e}")
            return self._generate_fallback_analysis(title, content)
    
    def _generate_fallback_analysis(self, title, content):
        """ç”Ÿæˆå¤‡ç”¨åˆ†æå†…å®¹"""
        return {
            "analysis_title": f"æ·±åº¦è§£è¯»ï¼š{title}",
            "executive_summary": f"æœ¬æ–‡æ·±å…¥åˆ†æäº†{title}çš„å¸‚åœºå½±å“å’ŒæŠ•èµ„å«ä¹‰ï¼Œä¸ºæŠ•èµ„è€…æä¾›ä¸“ä¸šçš„å†³ç­–å‚è€ƒã€‚",
            "market_analysis": {
                "immediate_impact": "è¯¥æ¶ˆæ¯å¯¹å¸‚åœºäº§ç”Ÿäº†å³æ—¶å½±å“ï¼ŒæŠ•èµ„è€…éœ€è¦å¯†åˆ‡å…³æ³¨ç›¸å…³æ¿å—çš„è¡¨ç°ã€‚",
                "long_term_implications": "ä»é•¿æœŸæ¥çœ‹ï¼Œè¿™ä¸€äº‹ä»¶å¯èƒ½ä¼šæ”¹å˜è¡Œä¸šæ ¼å±€ï¼Œå½±å“ç›¸å…³å…¬å¸çš„åŸºæœ¬é¢ã€‚",
                "affected_sectors": [
                    {
                        "sector": "ç›¸å…³è¡Œä¸š",
                        "impact_level": "ä¸­",
                        "key_companies": ["å¾…åˆ†æ"],
                        "analysis": "éœ€è¦è¿›ä¸€æ­¥è§‚å¯Ÿå¸‚åœºååº”"
                    }
                ]
            },
            "investment_perspective": {
                "opportunities": "å¸‚åœºæ³¢åŠ¨ä¸­å¾€å¾€è•´å«æŠ•èµ„æœºä¼šï¼Œå»ºè®®å…³æ³¨åŸºæœ¬é¢è‰¯å¥½çš„ä¼˜è´¨æ ‡çš„ã€‚",
                "risks": "æŠ•èµ„è€…åº”æ³¨æ„å¸‚åœºé£é™©ï¼Œåšå¥½é£é™©ç®¡ç†å’Œèµ„äº§é…ç½®ã€‚",
                "strategy_suggestions": "å»ºè®®é‡‡ç”¨åˆ†æ•£æŠ•èµ„ç­–ç•¥ï¼Œå…³æ³¨é•¿æœŸä»·å€¼æŠ•èµ„æœºä¼šã€‚"
            },
            "technical_analysis": {
                "key_indicators": "å…³æ³¨æˆäº¤é‡å’Œä»·æ ¼èµ°åŠ¿çš„é…åˆæƒ…å†µ",
                "price_targets": "æ ¹æ®æŠ€æœ¯åˆ†æç¡®å®šåˆç†çš„ä»·æ ¼ç›®æ ‡",
                "support_resistance": "è¯†åˆ«å…³é”®çš„æ”¯æ’‘å’Œé˜»åŠ›ä½"
            },
            "conclusion": "ç»¼åˆåˆ†ææ˜¾ç¤ºï¼ŒæŠ•èµ„è€…åº”ä¿æŒç†æ€§ï¼ŒåŸºäºåŸºæœ¬é¢åˆ†æåšå‡ºæŠ•èµ„å†³ç­–ã€‚",
            "tags": ["è´¢ç»åˆ†æ", "å¸‚åœºè§£è¯»", "æŠ•èµ„ç­–ç•¥", "é£é™©ç®¡ç†", "ä»·å€¼æŠ•èµ„"],
            "seo_keywords": ["è´¢ç»", "æŠ•èµ„", "å¸‚åœºåˆ†æ"],
            "risk_disclaimer": "æœ¬åˆ†æä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚æŠ•èµ„æœ‰é£é™©ï¼Œå…¥å¸‚éœ€è°¨æ…ã€‚",
            "content_quality_score": 85,
            "originality_score": 90,
            "generated_at": datetime.now().isoformat(),
            "ai_model": "fallback",
            "analysis_version": "2.0"
        }
    
    def analyze_market_news(self, text, title=None, searxng_results=None):
        """å…¼å®¹æ—§æ¥å£çš„å¸‚åœºæ–°é—»åˆ†æ"""
        return self.generate_comprehensive_analysis(title or "å¸‚åœºæ–°é—»", text, searxng_results)
    
    def analyze_article(self, article_data):
        """å…¼å®¹æ—§æ¥å£çš„æ–‡ç« åˆ†ææ–¹æ³•"""
        title = article_data.get('title', '')
        content = article_data.get('content', '')
        search_results = article_data.get('search_results', [])
        
        return self.generate_comprehensive_analysis(
            title=title,
            content=content,
            search_results=search_results
        )
