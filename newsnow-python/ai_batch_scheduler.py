#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIæ‰¹é‡åˆ†æå®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨
ä¸“é—¨å¤„ç†æœªåˆ†æçš„æ–‡ç« ï¼Œç¡®ä¿æ‰€æœ‰æ–‡ç« éƒ½æœ‰AIåˆ†æå†…å®¹
"""

import os
import sys
import time
import schedule
import logging
import argparse
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from batch_ai_processor import BatchAIProcessor
from config.settings import LOG_LEVEL, LOG_DIR

def setup_logging():
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    os.makedirs(LOG_DIR, exist_ok=True)
    log_file = os.path.join(LOG_DIR, f"ai_batch_scheduler-{datetime.now().strftime('%Y%m%d')}.log")
    
    logging.basicConfig(
        level=getattr(logging, LOG_LEVEL),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )
    
    return logging.getLogger(__name__)

def ai_batch_processing_job(logger, batch_size=5, max_batches=10):
    """
    AIæ‰¹é‡å¤„ç†ä»»åŠ¡
    
    Args:
        logger: æ—¥å¿—è®°å½•å™¨
        batch_size (int): æ¯æ‰¹å¤„ç†çš„æ–‡ç« æ•°é‡
        max_batches (int): æœ€å¤§æ‰¹æ¬¡æ•°é‡ï¼ˆé˜²æ­¢å•æ¬¡è¿è¡Œæ—¶é—´è¿‡é•¿ï¼‰
    """
    logger.info(f"===== å¼€å§‹AIæ‰¹é‡å¤„ç†ä»»åŠ¡: {datetime.now().isoformat()} =====")
    
    try:
        processor = BatchAIProcessor()
        
        # é™åˆ¶å•æ¬¡è¿è¡Œçš„æ‰¹æ¬¡æ•°é‡ï¼Œé¿å…è¿è¡Œæ—¶é—´è¿‡é•¿
        total_processed = 0
        total_success = 0
        batch_count = 0
        
        while batch_count < max_batches:
            # è·å–æœªåˆ†æçš„æ–‡ç« 
            from db.sqlite_client import SQLiteClient
            db_client = SQLiteClient()
            unanalyzed_articles = db_client.get_unprocessed_articles(limit=batch_size)
            
            if not unanalyzed_articles:
                logger.info("âœ… æ²¡æœ‰éœ€è¦åˆ†æçš„æ–‡ç« ï¼Œä»»åŠ¡å®Œæˆ")
                break
            
            batch_count += 1
            logger.info(f"ğŸ“¦ å¤„ç†ç¬¬ {batch_count} æ‰¹ ({len(unanalyzed_articles)} ç¯‡æ–‡ç« )...")
            
            batch_success = 0
            for article in unanalyzed_articles:
                try:
                    article_id = article.get('id')
                    title = article.get('title', '')
                    content = article.get('content', '')
                    
                    logger.info(f"ğŸ” åˆ†ææ–‡ç« : {title[:30]}...")
                    
                    # æ‰§è¡ŒAIåˆ†æ
                    analysis_result = processor.analyzer.generate_comprehensive_analysis(
                        title=title,
                        content=content,
                        search_results=[]
                    )
                    
                    if analysis_result:
                        # æ›´æ–°æ•°æ®åº“
                        success = db_client.update_article_analysis(article_id, analysis_result)
                        if success:
                            logger.info(f"âœ… åˆ†æå®Œæˆ: {title[:20]}...")
                            batch_success += 1
                            total_success += 1
                        else:
                            logger.warning(f"âš ï¸ ä¿å­˜å¤±è´¥: {title[:20]}...")
                    else:
                        logger.warning(f"âš ï¸ åˆ†æå¤±è´¥: {title[:20]}...")
                    
                    total_processed += 1
                    
                    # æ–‡ç« é—´çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…APIé¢‘ç‡è¿‡é«˜
                    time.sleep(3)
                    
                except Exception as e:
                    logger.error(f"âŒ å¤„ç†æ–‡ç« å¼‚å¸¸: {e}")
                    continue
            
            logger.info(f"ğŸ“Š ç¬¬ {batch_count} æ‰¹å®Œæˆ: {batch_success}/{len(unanalyzed_articles)} æˆåŠŸ")
            
            # æ‰¹æ¬¡é—´å»¶è¿Ÿ
            if batch_count < max_batches and len(unanalyzed_articles) == batch_size:
                logger.info("â° ç­‰å¾…60ç§’åå¤„ç†ä¸‹ä¸€æ‰¹...")
                time.sleep(60)
        
        logger.info(f"ğŸ“ˆ æœ¬æ¬¡ä»»åŠ¡å®Œæˆ: å¤„ç† {total_processed} ç¯‡, æˆåŠŸ {total_success} ç¯‡")
        
        if batch_count >= max_batches:
            logger.info(f"â° è¾¾åˆ°æœ€å¤§æ‰¹æ¬¡é™åˆ¶ ({max_batches})ï¼Œä¸‹æ¬¡è°ƒåº¦æ—¶ç»§ç»­å¤„ç†")
            
    except Exception as e:
        logger.error(f"âŒ AIæ‰¹é‡å¤„ç†ä»»åŠ¡å¼‚å¸¸: {str(e)}")
    
    logger.info(f"===== AIæ‰¹é‡å¤„ç†ä»»åŠ¡ç»“æŸ =====\n")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='AIæ‰¹é‡åˆ†æå®šæ—¶ä»»åŠ¡')
    parser.add_argument('--once', action='store_true', help='ä»…è¿è¡Œä¸€æ¬¡ï¼Œä¸å¯åŠ¨å®šæ—¶ä»»åŠ¡')
    parser.add_argument('--interval', type=int, default=20, help='ä»»åŠ¡é—´éš”ï¼ˆåˆ†é’Ÿï¼‰')
    parser.add_argument('--batch-size', type=int, default=5, help='æ¯æ‰¹å¤„ç†çš„æ–‡ç« æ•°é‡')
    parser.add_argument('--max-batches', type=int, default=10, help='å•æ¬¡è¿è¡Œçš„æœ€å¤§æ‰¹æ¬¡æ•°')
    args = parser.parse_args()
    
    # æ‰‹åŠ¨åŠ è½½ç¯å¢ƒå˜é‡
    env_file = ".env"
    if os.path.exists(env_file):
        try:
            with open(env_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and '=' in line:
                        key, value = line.split('=', 1)
                        os.environ[key.strip()] = value.strip()
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ç¯å¢ƒå˜é‡å¤±è´¥: {e}")
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging()
    logger.info(f"AIæ‰¹é‡åˆ†æè°ƒåº¦å™¨å¯åŠ¨")
    logger.info(f"é…ç½®: é—´éš”={args.interval}åˆ†é’Ÿ, æ‰¹é‡å¤§å°={args.batch_size}, æœ€å¤§æ‰¹æ¬¡={args.max_batches}")
    
    # ç«‹å³æ‰§è¡Œä¸€æ¬¡
    ai_batch_processing_job(logger, args.batch_size, args.max_batches)
    
    # å¦‚æœåªè¿è¡Œä¸€æ¬¡ï¼Œç›´æ¥é€€å‡º
    if args.once:
        logger.info("æŒ‰ç…§å‚æ•°è¦æ±‚ï¼Œä»…è¿è¡Œä¸€æ¬¡ï¼Œç¨‹åºé€€å‡º")
        return
    
    # è®¾ç½®å®šæ—¶ä»»åŠ¡
    schedule.every(args.interval).minutes.do(
        ai_batch_processing_job, logger, args.batch_size, args.max_batches
    )
    logger.info(f"AIæ‰¹é‡å¤„ç†ä»»åŠ¡å·²è®¾ç½®ï¼Œæ¯ {args.interval} åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡")
    
    # è¿è¡Œå®šæ—¶ä»»åŠ¡
    logger.info(f"å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨")
    try:
        while True:
            schedule.run_pending()
            time.sleep(1)
    except KeyboardInterrupt:
        logger.info("æ”¶åˆ°ç»ˆæ­¢ä¿¡å·ï¼Œç¨‹åºé€€å‡º")
    except Exception as e:
        logger.error(f"ç¨‹åºå¼‚å¸¸: {str(e)}")
        raise

if __name__ == "__main__":
    main() 